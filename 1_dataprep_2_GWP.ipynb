{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "In this notebook, one can find the steps I used to prepare the data.\n",
    "The original data is in zip files in .sav format. I clean some of the unwanted columns here and save the data on year basis in pickled dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dataprep\n",
    "\n",
    "source = \"meta/columns\"\n",
    "df_meta = pd.read_pickle(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose here the source file to use\n",
    "# num = 1 # 2008-2012\n",
    "# num = 2 # 2013-2017\n",
    "# num = 3 # 2018-2022\n",
    "\n",
    "nums = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in tqdm(nums): \n",
    "    print(f\"########## Processing file {num} ##########\")   \n",
    "    # Reading the files\n",
    "    df, meta = dataprep.read_file(num)\n",
    "\n",
    "    # Data preparation\n",
    "\n",
    "    # Regions\n",
    "    df = dataprep.new_region_column(df)\n",
    "\n",
    "    # Renaming the features\n",
    "    df = dataprep.rename_with_codes(df, meta)\n",
    "\n",
    "    # Removing unwanted columns\n",
    "    df = dataprep.remove_unwanted(df, df_meta)\n",
    "\n",
    "    # Remove questions which are not asked in all countries. \n",
    "    df = dataprep.remove_notallcountry(df)\n",
    "    try:\n",
    "        df.drop('INDEX_CA: Community Attachment Index', axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Save files to pickles\n",
    "    dataprep.save_files(df, num, 'clean_per_year', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further preparation: Migration aspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in tqdm(nums):\n",
    "    print(f\"########## Processing file {num} ##########\")   \n",
    "    # reading files\n",
    "    if num == 1:\n",
    "        source = \"gwp_data/clean_per_year/clean_data_from8to12_\"\n",
    "    if num == 2:\n",
    "        source = \"gwp_data/clean_per_year/clean_data_from13to17_\"\n",
    "    if num == 3:\n",
    "        source = \"gwp_data/clean_per_year/clean_data_from18to22_\"\n",
    "        \n",
    "    df_aspiration = pd.read_pickle(source)\n",
    "\n",
    "    # Check for duplicate columns and keep only the columns that are not duplicates\n",
    "    duplicated_columns = df_aspiration.columns.duplicated()\n",
    "    df_aspiration = df_aspiration.loc[:, ~duplicated_columns]\n",
    "\n",
    "    # remove lines where the answer is not yes or no\n",
    "    df_aspiration = dataprep.remove_aspiration_DK(df_aspiration)\n",
    "\n",
    "    # impute values by type\n",
    "    df_aspiration = dataprep.impute_missing_by_type(df_aspiration, df_meta)\n",
    "\n",
    "    print(f\"Shape: {df_aspiration.shape}\")\n",
    "    # Check which columns have missing values\n",
    "    columns_with_missing_values = df_aspiration.columns[df_aspiration.isnull().any()]\n",
    "\n",
    "    print(f\"Shape before dropna: {df_aspiration.shape}\")\n",
    "    df_aspiration.dropna(inplace=True)\n",
    "\n",
    "    print(f\"Shape before sampling: {df_aspiration.shape}\")\n",
    "    df_aspiration = dataprep.sampling(df_aspiration)\n",
    "\n",
    "    df_aspiration['index'] = (range(len(df_aspiration['WP1220: Age'])))\n",
    "    df_aspiration.set_index('index', inplace=True)\n",
    "\n",
    "    print(f\"Shape of the dataframe: {df_aspiration.shape}\")\n",
    "\n",
    "    # save files\n",
    "    dataprep.save_files(df_aspiration, num, 'prepared_aspiration', '')\n",
    "\n",
    "    df_aspiration_without_regions = df_aspiration.drop([\"REG2_GLOBAL: Region 2 Global\",\"REG_GLOBAL: Global Region\"], axis=1)\n",
    "\n",
    "    # save files without regions\n",
    "    dataprep.save_files(df_aspiration_without_regions, num, 'prepared_aspiration', 'woregions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further preparation: migration destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in tqdm(nums):\n",
    "    print(f\"########## Processing file {num} ##########\")   \n",
    "    # reading files\n",
    "    if num == 1:\n",
    "        source = \"gwp_data/clean_per_year/clean_data_from8to12_\"\n",
    "    if num == 2:\n",
    "        source = \"gwp_data/clean_per_year/clean_data_from13to17_\"\n",
    "    if num == 3:\n",
    "        source = \"gwp_data/clean_per_year/clean_data_from18to22_\"    \n",
    "    df_destination = pd.read_pickle(source)\n",
    "\n",
    "    # Check for duplicate columns and keep only the columns that are not duplicates\n",
    "    duplicated_columns = df_destination.columns.duplicated()\n",
    "    df_destination = df_destination.loc[:, ~duplicated_columns]\n",
    "\n",
    "    region_cols = ['REG2_GLOBAL: Region 2 Global', 'REG_GLOBAL: Global Region']\n",
    "    df_destination.drop(columns = region_cols, axis =1,  inplace = True)\n",
    "\n",
    "    # remove rows where destination is missing\n",
    "    df_destination = df_destination[df_destination['WP3120: Country Would Move To'].notnull()]\n",
    "\n",
    "    # impute values by type\n",
    "    df_destination = dataprep.impute_missing_by_type(df_destination, df_meta)\n",
    "\n",
    "    # some columns are not imputed\n",
    "    df_destination = df_destination[df_destination['WP1220: Age'].notna()]\n",
    "\n",
    "    dataprep.save_files(df_destination, num, 'prepared_destination', '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge aspiration for each year\n",
    "df_asp = pd.DataFrame()\n",
    "df_asp_woregions = pd.DataFrame()\n",
    "df_dest = pd.DataFrame()\n",
    "# df_clean = pd.DataFrame()\n",
    "\n",
    "for year in range(2008, 2023):\n",
    "\n",
    "    df_asp_year = pd.read_pickle(f\"gwp_data/prepared_aspiration/clean_data_{year}_\")\n",
    "    df_asp_year_woregions = pd.read_pickle(f\"gwp_data/prepared_aspiration/clean_data_{year}_woregions\")\n",
    "    df_dest_year = pd.read_pickle(f\"gwp_data/prepared_destination/clean_data_{year}_\")\n",
    "\n",
    "    df_asp = pd.concat([df_asp, df_asp_year], ignore_index=True)\n",
    "    df_asp_woregions = pd.concat([df_asp_woregions, df_asp_year_woregions], ignore_index=True)\n",
    "    df_dest = pd.concat([df_dest, df_dest_year], ignore_index=True)\n",
    "\n",
    "df_asp.to_pickle('gwp_data/prepared_aspiration/full_aspiration')\n",
    "df_asp_woregions.to_pickle('gwp_data/prepared_aspiration/full_aspiration_woregions')\n",
    "df_dest.to_pickle('gwp_data/prepared_destination/full_destination')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem-pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "041b256c5c99f7718f2c855727968e49e7d3f10987ef9aa096be837926238695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
